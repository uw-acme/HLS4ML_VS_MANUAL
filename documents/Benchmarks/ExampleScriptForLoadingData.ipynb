{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 17:25:01.128630: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-03 17:25:01.129856: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/carolinejohnson/miniconda3/envs/weird/lib/python3.8/site-packages/hls4ml/converters/__init__.py:16: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\")\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('encoder_8x8_c8_S2_tele.json', 'r')\n",
    "loaded_model_json = json_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 17:25:30.358095: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-10-03 17:25:30.389587: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-10-03 17:25:30.389628: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-10-03 17:25:30.389680: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2023-10-03 17:25:30.392776: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8, 8, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 4, 4, 8)           80        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "encoded_vector (Dense)       (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 2,144\n",
      "Trainable params: 2,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights('encoder_8x8_c8_S2_tele.h5')\n",
    "loaded_model.get_weights()\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loaded_model.load_weights('encoder_8x8_c8_S2_tele.h5')\n",
    "# x = np.array([[[[0, 0,  0,  0,\n",
    "#                 0, 0,  0,  0 ]],\n",
    " \n",
    "#          [[ 0,   0,  0,  0,\n",
    "#                 0, 0,  0,  0 ]],\n",
    " \n",
    "#          [[ 0,  0,  0,  0,\n",
    "#                 0, 0,  0,  0 ]]],\n",
    " \n",
    " \n",
    "#         [[[ 0,  0,  0,  0,\n",
    "#                 0, 0,  0,  0 ]],\n",
    " \n",
    "#          [[0,  0,  0,  0,\n",
    "#                 0, 0,  0,  0 ]],\n",
    " \n",
    "#          [[0,  0,  0,  0,\n",
    "#                 0, 0,  0,  0 ]]],\n",
    " \n",
    " \n",
    "#         [[[0,  0,  0,  0,\n",
    "#                 0, 0,  0,  0 ]],\n",
    " \n",
    "#          [[0 , 0,  0,  0,\n",
    "#                 0, 0,  0,  0 ]],\n",
    " \n",
    "#          [[0,  0,  0,  0,\n",
    "#                 0, 0,  0,  0 ]]]],\n",
    "#       dtype=\"float32\")\n",
    "# y = np. array([ 0,  0,  0,  0,  0,\n",
    "#         0, 0,  0], dtype=\"float32\")\n",
    "# #loaded_model.summary()\n",
    "# l = []\n",
    "# l.append(x)\n",
    "# l.append(y)\n",
    "# loaded_model.layers[1].set_weights(l)\n",
    "# loaded_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: conv2d, layer type: Conv2D\n",
      "  -> Activation (relu), layer name: conv2d\n",
      "Layer name: encoded_vector, layer type: Dense\n",
      "  -> Activation (relu), layer name: encoded_vector\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 8, 8, 1]], output shape: [None, 8, 8, 1]\n",
      "Layer name: conv2d, layer type: Conv2D, input shapes: [[None, 8, 8, 1]], output shape: [None, 4, 4, 8]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 4, 4, 8]], output shape: [None, 128]\n",
      "Layer name: encoded_vector, layer type: Dense, input shapes: [[None, 128]], output shape: [None, 16]\n",
      "Creating HLS model\n",
      "WARNING: Strategy for layer input_1 set to \"Resource\", while model strategy set to \"Latency\".\n",
      "WARNING: Strategy for layer conv2d set to \"Resource\", while model strategy set to \"Latency\".\n",
      "WARNING: Strategy for layer conv2d_relu set to \"Resource\", while model strategy set to \"Latency\".\n",
      "WARNING: Strategy for layer encoded_vector set to \"Resource\", while model strategy set to \"Latency\".\n",
      "WARNING: Strategy for layer encoded_vector_relu set to \"Resource\", while model strategy set to \"Latency\".\n",
      "WARNING: Changing model strategy to \"Resource\"\n",
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#First, the baseline model\n",
    "hls_config = hls4ml.utils.config_from_keras_model(loaded_model, granularity='name')\n",
    "\n",
    "# Set the precision and reuse factor for the full model\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<16,6>'\n",
    "hls_config['Model']['ReuseFactor'] = 1\n",
    "\n",
    "# Create an entry for each layer, here you can for instance change the strategy for a layer to 'resource' \n",
    "# or increase the reuse factor individually for large layers.\n",
    "# In this case, we designed the model to be small enough for a fully parallel implementation \n",
    "# so we use the latency strategy and reuse factor of 1 for all layers.\n",
    "for Layer in hls_config['LayerName'].keys():\n",
    "    hls_config['LayerName'][Layer]['Strategy'] = 'Resource'\n",
    "    hls_config['LayerName'][Layer]['ReuseFactor'] = 3\n",
    "#If you want best numerical performance for high-accuray models, while the default latency strategy is faster but numerically more unstable\n",
    "#hls_config['LayerName']['output_softmax']['Strategy'] = 'Stable'\n",
    "#plotting.print_dict(hls_config)\n",
    "\n",
    "hls_config['LayerName'][\"encoded_vector\"]['ReuseFactor'] = 2\n",
    "\n",
    "cfg = hls4ml.converters.create_config(backend='Vivado')\n",
    "cfg['IOType']     = 'io_stream' # Must set this if using CNNs!\n",
    "cfg['HLSConfig']  = hls_config\n",
    "cfg['KerasModel'] = loaded_model\n",
    "cfg['OutputDir']  = 'testingIOType'\n",
    "cfg['XilinxPart'] = 'xcu250-figd2104-2L-e'\n",
    "  \n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8, 8, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 4, 4, 8)           80        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "encoded_vector (Dense)       (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 2,144\n",
      "Trainable params: 2,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[[[ 1.43463641e-01, -5.92209935e-01,  1.12317359e+00,\n",
       "           -2.12841667e-03,  2.08273139e-02,  5.73347986e-01,\n",
       "            8.00394937e-02,  1.12851836e-01]],\n",
       " \n",
       "         [[ 2.47702882e-01, -3.82787645e-01,  8.97283316e-01,\n",
       "            4.29426938e-01, -4.47546810e-01, -4.98714261e-02,\n",
       "            5.73822796e-01, -4.58222851e-02]],\n",
       " \n",
       "         [[-1.36033118e-01,  4.74251397e-02,  4.58531491e-02,\n",
       "            6.99015036e-02, -1.37546152e-01,  2.65467882e-01,\n",
       "            2.52627492e-01, -3.66227739e-02]]],\n",
       " \n",
       " \n",
       "        [[[-1.77963480e-01, -6.15566432e-01,  1.02063775e+00,\n",
       "            3.40583533e-01, -2.27287635e-01,  3.44268158e-02,\n",
       "            4.10290547e-02,  2.89543450e-01]],\n",
       " \n",
       "         [[-2.18355134e-02, -1.94294661e-01,  7.73878455e-01,\n",
       "            9.81672823e-01, -8.73244047e-01,  4.17940207e-02,\n",
       "            3.96928005e-02, -1.99224949e-01]],\n",
       " \n",
       "         [[ 4.33081314e-02,  1.86833858e-01, -1.83565430e-02,\n",
       "            6.04253769e-01, -5.46070933e-01,  5.63281894e-01,\n",
       "           -1.07377231e-01,  7.04304576e-02]]],\n",
       " \n",
       " \n",
       "        [[[-1.80864498e-01,  4.34804522e-02, -1.78417619e-02,\n",
       "            3.31942976e-01, -2.73086727e-01,  3.34052257e-02,\n",
       "           -4.98984866e-02,  2.25733630e-02]],\n",
       " \n",
       "         [[-5.93441837e-02,  7.65857920e-02, -1.89910177e-02,\n",
       "            4.17138815e-01, -2.65201479e-01, -2.78147869e-02,\n",
       "           -2.43037604e-02, -1.26128227e-01]],\n",
       " \n",
       "         [[-1.01630576e-02,  6.32169247e-02,  1.34485755e-02,\n",
       "            1.27825767e-01, -1.36095926e-01, -7.89970718e-03,\n",
       "            3.69431335e-04,  2.66104918e-02]]]], dtype=float32),\n",
       " array([ 0.13968787,  0.14291538,  0.00359119,  0.00152325,  0.13942908,\n",
       "        -0.08317281, -0.08828932,  0.04472383], dtype=float32),\n",
       " array([[ 0.04618631,  0.00720111,  0.7708136 , ...,  0.4151419 ,\n",
       "          0.21640716, -0.4350931 ],\n",
       "        [ 0.15225321,  0.60930675,  0.41481256, ..., -0.62820345,\n",
       "          0.32734826,  0.2874379 ],\n",
       "        [-0.90754503,  0.48139974, -0.15553162, ..., -0.03224144,\n",
       "         -2.7868774 , -0.7620896 ],\n",
       "        ...,\n",
       "        [-0.13873415, -0.08787181, -0.09337988, ..., -0.13591766,\n",
       "         -0.13132213,  0.16199426],\n",
       "        [-0.14256406,  0.21180825, -0.00651214, ..., -0.18499903,\n",
       "          0.18035986,  0.11822663],\n",
       "        [ 0.13018641,  0.37040204,  0.27661717, ..., -0.14037174,\n",
       "          0.2394648 ,  0.03918015]], dtype=float32),\n",
       " array([ 0.33346647,  0.25099307,  0.12052044,  0.06681135, -0.12547506,\n",
       "         0.2315379 , -0.07396878,  0.21925628,  0.39043963,  0.32542816,\n",
       "         0.21592437,  0.1426833 , -0.03305586,  0.14063242,  0.22496955,\n",
       "         0.12423215], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.summary()\n",
    "loaded_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OutputDir': 'testingIOType',\n",
       " 'ProjectName': 'myproject',\n",
       " 'Backend': 'Vivado',\n",
       " 'XilinxPart': 'xcu250-figd2104-2L-e',\n",
       " 'Board': None,\n",
       " 'ClockPeriod': 5,\n",
       " 'IOType': 'io_stream',\n",
       " 'HLSConfig': {'Model': {'Precision': 'ap_fixed<16,6>',\n",
       "   'ReuseFactor': 1,\n",
       "   'Strategy': 'Latency'},\n",
       "  'LayerName': {'input_1': {'Precision': {'result': 'ap_fixed<16,6>'},\n",
       "    'Strategy': 'Resource',\n",
       "    'ReuseFactor': 3},\n",
       "   'conv2d': {'Precision': {'weight': 'ap_fixed<16,6>',\n",
       "     'bias': 'ap_fixed<16,6>',\n",
       "     'result': 'ap_fixed<16,6>'},\n",
       "    'ReuseFactor': 3,\n",
       "    'Strategy': 'Resource',\n",
       "    'IOType': 'io_parallel'},\n",
       "   'conv2d_relu': {'Precision': 'ap_fixed<16,6>',\n",
       "    'ReuseFactor': 3,\n",
       "    'table_size': 1024,\n",
       "    'table_t': 'ap_fixed<18,8>',\n",
       "    'Strategy': 'Resource'},\n",
       "   'encoded_vector': {'Precision': {'weight': 'ap_fixed<16,6>',\n",
       "     'bias': 'ap_fixed<16,6>',\n",
       "     'result': 'ap_fixed<16,6>'},\n",
       "    'ReuseFactor': 2,\n",
       "    'Strategy': 'Resource'},\n",
       "   'encoded_vector_relu': {'Precision': 'ap_fixed<16,6>',\n",
       "    'ReuseFactor': 3,\n",
       "    'table_size': 1024,\n",
       "    'table_t': 'ap_fixed<18,8>',\n",
       "    'Strategy': 'Resource'}},\n",
       "  'Strategy': 'Latency'},\n",
       " 'KerasModel': <tensorflow.python.keras.engine.functional.Functional at 0x7fa0374d5160>,\n",
       " 'Stamp': 'a53BA2dc'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['HLSConfig']['LayerName']['conv2d']['IOType'] = 'io_parallel'\n",
    "cfg['HLSConfig']['Strategy'] = 'Latency'\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OutputDir': 'testingRegular',\n",
       " 'ProjectName': 'myproject',\n",
       " 'Backend': 'Vivado',\n",
       " 'XilinxPart': 'xcu250-figd2104-2L-e',\n",
       " 'Board': None,\n",
       " 'ClockPeriod': 5,\n",
       " 'IOType': 'io_stream',\n",
       " 'HLSConfig': {'Model': {'Precision': 'ap_fixed<16,6>',\n",
       "   'ReuseFactor': 1,\n",
       "   'Strategy': 'Latency'},\n",
       "  'LayerName': {'input_1': {'Precision': {'result': 'ap_fixed<16,6>'},\n",
       "    'Strategy': 'Resource',\n",
       "    'ReuseFactor': 3},\n",
       "   'conv2d': {'Precision': {'weight': 'ap_fixed<16,6>',\n",
       "     'bias': 'ap_fixed<16,6>',\n",
       "     'result': 'ap_fixed<16,6>'},\n",
       "    'ReuseFactor': 3,\n",
       "    'Strategy': 'Resource'},\n",
       "   'conv2d_relu': {'Precision': 'ap_fixed<16,6>',\n",
       "    'ReuseFactor': 3,\n",
       "    'table_size': 1024,\n",
       "    'table_t': 'ap_fixed<18,8>',\n",
       "    'Strategy': 'Resource'},\n",
       "   'encoded_vector': {'Precision': {'weight': 'ap_fixed<16,6>',\n",
       "     'bias': 'ap_fixed<16,6>',\n",
       "     'result': 'ap_fixed<16,6>'},\n",
       "    'ReuseFactor': 2,\n",
       "    'Strategy': 'Resource'},\n",
       "   'encoded_vector_relu': {'Precision': 'ap_fixed<16,6>',\n",
       "    'ReuseFactor': 3,\n",
       "    'table_size': 1024,\n",
       "    'table_t': 'ap_fixed<18,8>',\n",
       "    'Strategy': 'Resource'}}},\n",
       " 'KerasModel': <tensorflow.python.keras.engine.functional.Functional at 0x7fa0374d5160>,\n",
       " 'Stamp': 'Cdedd75C'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2.1 (64-bit)\n",
      "  **** SW Build 2729669 on Thu Dec  5 04:48:12 MST 2019\n",
      "  **** IP Build 2729494 on Thu Dec  5 07:38:25 MST 2019\n",
      "    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source /tools/Xilinx/Vivado/2019.2/scripts/vivado_hls/hls.tcl -notrace\n",
      "INFO: [HLS 200-10] Running '/tools/Xilinx/Vivado/2019.2/bin/unwrapped/lnx64.o/vivado_hls'\n",
      "INFO: [HLS 200-10] For user 'carolinejohnson' on host 'ubuntu' (Linux_x86_64 version 5.4.0-150-generic) on Tue Oct 03 17:30:17 PDT 2023\n",
      "INFO: [HLS 200-10] On os Ubuntu 18.04.6 LTS\n",
      "INFO: [HLS 200-10] In directory '/home/carolinejohnson/mkconfig/testingIOType'\n",
      "Sourcing Tcl script 'build_prj.tcl'\n",
      "INFO: [HLS 200-10] Creating and opening project '/home/carolinejohnson/mkconfig/testingIOType/myproject_prj'.\n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project\n",
      "INFO: [HLS 200-10] Creating and opening solution '/home/carolinejohnson/mkconfig/testingIOType/myproject_prj/solution1'.\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 60.\n",
      "INFO: [HLS 200-10] Setting target device to 'xcu250-figd2104-2L-e'\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with a period of 5ns.\n",
      "***** C/RTL SYNTHESIS *****\n",
      "INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.\n",
      "INFO: [HLS 200-10] Analyzing design file 'firmware/myproject.cpp' ... \n",
      "WARNING: [HLS 214-114] Since the only kind of statements allowed in a canonical dataflow region are variable declarations and function calls, the compiler may not be able to correctly handle the region: firmware/nnet_utils/nnet_dense_latency.h:64:9\n",
      "WARNING: [HLS 214-114] Since the only kind of statements allowed in a canonical dataflow region are variable declarations and function calls, the compiler may not be able to correctly handle the region: firmware/nnet_utils/nnet_dense_latency.h:79:2\n",
      "WARNING: [HLS 214-104] Only for-loops and functions support the dataflow: firmware/nnet_utils/nnet_dense_latency.h:76:9\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:62:72\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:62:76\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:70:67\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:70:71\n",
      "WARNING: [HLS 214-114] Since the only kind of statements allowed in a canonical dataflow region are variable declarations and function calls, the compiler may not be able to correctly handle the region: firmware/myproject.cpp:35:2\n",
      "WARNING: [HLS 214-114] Since the only kind of statements allowed in a canonical dataflow region are variable declarations and function calls, the compiler may not be able to correctly handle the region: firmware/myproject.cpp:36:5\n",
      "WARNING: [HLS 200-471] Dataflow form checks found 9 issue(s) in file firmware/myproject.cpp\n",
      "INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:27 ; elapsed = 00:00:45 . Memory (MB): peak = 891.867 ; gain = 459.090 ; free physical = 410 ; free virtual = 2455\n",
      "INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:27 ; elapsed = 00:00:45 . Memory (MB): peak = 891.867 ; gain = 459.090 ; free physical = 410 ; free virtual = 2455\n",
      "INFO: [HLS 200-10] Starting code transformations ...\n",
      "INFO: [HLS 200-489] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:220) in function 'void nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>(FORWARD_REFERENCE const&, ap_shift_reg<FORWARD_REFERENCE::value_type, FORWARD_REFERENCE::in_width> (*) [FORWARD_REFERENCE::n_chan], FORWARD_REFERENCE::value_type*)' completely with a factor of 1.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_padding_stream.h:12) in function 'void nnet::fill_zero<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>(hls::stream<FORWARD_REFERENCE>&)' completely with a factor of 1.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_padding_stream.h:24) in function 'void nnet::fill_data<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>(hls::stream<FORWARD_REFERENCE>&, hls::stream<FORWARD_REFERENCE>&)' completely with a factor of 1.\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>::operator[].1' into 'nnet::fill_zero<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>' (firmware/nnet_utils/nnet_padding_stream.h:14).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>::operator[].1' into 'nnet::fill_data<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>' (firmware/nnet_utils/nnet_padding_stream.h:26).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>::operator[].1' into 'nnet::fill_data<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>' (firmware/nnet_utils/nnet_padding_stream.h:26).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::fill_data<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>' into 'nnet::zeropad2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>' (firmware/nnet_utils/nnet_padding_stream.h:66).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::fill_zero<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>' into 'nnet::zeropad2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>' (firmware/nnet_utils/nnet_padding_stream.h:75).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::fill_zero<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>' into 'nnet::zeropad2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>' (firmware/nnet_utils/nnet_padding_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>::operator[]' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:224).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::kernel_shift_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:235).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_resource_rf_leq_nin<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' (firmware/nnet_utils/nnet_dense_resource.h:76).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_resource_rf_leq_nin<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' (firmware/nnet_utils/nnet_dense_resource.h:76).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_resource_rf_leq_nin<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' into 'nnet::dense_resource<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' (firmware/nnet_utils/nnet_dense_resource.h:274).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:285).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_dense_stream.h:48).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' (firmware/nnet_utils/nnet_activation_stream.h:70).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_resource<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:279).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' into 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_conv2d_stream.h:85).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_conv2d_stream.h:103).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_resource_rf_leq_nin<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' into 'nnet::dense_resource<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' (firmware/nnet_utils/nnet_dense_resource.h:274).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_resource<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' into 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' (firmware/nnet_utils/nnet_dense_stream.h:24).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' (firmware/nnet_utils/nnet_activation_stream.h:70).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_dense_stream.h:62).\n",
      "INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:29 ; elapsed = 00:00:49 . Memory (MB): peak = 891.867 ; gain = 459.090 ; free physical = 372 ; free virtual = 2428\n",
      "INFO: [HLS 200-10] Checking synthesizability ...\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_dense_resource.h:99->firmware/nnet_utils/nnet_dense_resource.h:274->firmware/nnet_utils/nnet_conv_stream.h:279->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' into 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' (firmware/nnet_utils/nnet_dense_resource.h:99->firmware/nnet_utils/nnet_dense_resource.h:274->firmware/nnet_utils/nnet_dense_stream.h:24) automatically.\n",
      "INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:30 ; elapsed = 00:00:49 . Memory (MB): peak = 891.867 ; gain = 459.090 ; free physical = 345 ; free virtual = 2404\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:64) into a 128-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:64) into a 256-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_dense_stream.h:58) into a 256-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_conv_stream.h:265) into a 128-bit variable.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReLUActLoop' (firmware/nnet_utils/nnet_activation_stream.h:60) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' (firmware/nnet_utils/nnet_activation_stream.h:60:49).\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'DataPrepare' (firmware/nnet_utils/nnet_dense_stream.h:41) in function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:64) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:64) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:188:75).\n",
      "WARNING: [XFORM 203-505] Ignored pipeline directive for loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:188) because its parent loop or function is pipelined.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:67) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:67) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'DataPack' (firmware/nnet_utils/nnet_dense_stream.h:46) in function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResPack' (firmware/nnet_utils/nnet_dense_stream.h:60) in function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:58) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:73) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 24.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:97) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'CastLoop' (firmware/nnet_utils/nnet_conv_stream.h:283) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:58) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:73) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' completely with a factor of 1024.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:97) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:229) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:188) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:190) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' completely with a factor of 3.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:200) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' completely with a factor of 3.\n",
      "INFO: [XFORM 203-131] Reshaping array 'w2.V'  in dimension 1 with a block factor of 24.\n",
      "INFO: [XFORM 203-131] Reshaping array 'w5.V'  in dimension 1 with a block factor of 1024.\n",
      "INFO: [XFORM 203-102] Automatically partitioning shift register array 'line_buffer.Array.V' .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'input_1.V.data.V' (firmware/myproject.cpp:25) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer7_out.V.data.V' (firmware/myproject.cpp:56) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer2_out.V.data.V' (firmware/myproject.cpp:60) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer3_out.V.data.V' (firmware/myproject.cpp:64) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer5_out.V.data.V' (firmware/myproject.cpp:68) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer6_out.V.data.V' (firmware/myproject.cpp:26) .\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "WARNING: [XFORM 203-104] Completely partitioning array 'data.V' (firmware/nnet_utils/nnet_dense_stream.h:35) accessed through non-constant indices on dimension 1 (firmware/nnet_utils/nnet_dense_stream.h:48:39), which may result in long runtime and suboptimal QoR due to large multiplexers. Please consider wrapping the array access into a function or using a register file core instead.\n",
      "INFO: [XFORM 203-101] Partitioning array 'data.V' (firmware/nnet_utils/nnet_dense_stream.h:35) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'res'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'kernel_data.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'res_out.i.i'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b2.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_resource.h:54) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b5.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_resource.h:54) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:217) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'input_1.V.data.V' (firmware/myproject.cpp:25) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer7_out.V.data.V' (firmware/myproject.cpp:56) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer2_out.V.data.V' (firmware/myproject.cpp:60) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer3_out.V.data.V' (firmware/myproject.cpp:64) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer5_out.V.data.V' (firmware/myproject.cpp:68) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer6_out.V.data.V' (firmware/myproject.cpp:26) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V'  in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:217) in dimension 2 completely.\n",
      "WARNING: [XFORM 203-104] Completely partitioning array 'data.V' (firmware/nnet_utils/nnet_dense_stream.h:35) accessed through non-constant indices on dimension 1 (firmware/nnet_utils/nnet_dense_resource.h:76:20), which may result in long runtime and suboptimal QoR due to large multiplexers. Please consider wrapping the array access into a function or using a register file core instead.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_dense_resource.h:99->firmware/nnet_utils/nnet_dense_resource.h:274->firmware/nnet_utils/nnet_conv_stream.h:279->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' into 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' (firmware/nnet_utils/nnet_dense_resource.h:99->firmware/nnet_utils/nnet_dense_resource.h:274->firmware/nnet_utils/nnet_dense_stream.h:24) automatically.\n",
      "INFO: [XFORM 203-712] Applying dataflow to function 'myproject', detected/extracted 6 process function(s): \n",
      "\t 'Block__proc'\n",
      "\t 'nnet::zeropad2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>'\n",
      "\t 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>'\n",
      "\t 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>'.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation_stream.h:60:74) to (firmware/nnet_utils/nnet_activation_stream.h:60:68) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>'... converting 25 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_activation_stream.h:75:1) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>'... converting 49 basic blocks.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' (firmware/nnet_utils/nnet_dense_resource.h:24:20)...1024 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_dense_resource.h:64:37)...27 expression(s) balanced.\n",
      "INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:02:39 ; elapsed = 00:03:09 . Memory (MB): peak = 1083.867 ; gain = 651.090 ; free physical = 151 ; free virtual = 2106\n",
      "WARNING: [XFORM 203-542] Cannot flatten a loop nest 'ReadInputWidth' (firmware/nnet_utils/nnet_conv2d_stream.h:79:85) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' : \n",
      "\n",
      "the outer loop is not a perfect loop because there is nontrivial logic before entering the inner loop.\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_conv2d_stream.h:78:80) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>'.\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::zeropad2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config7>' to 'zeropad2d_cl<array,array<ap_fixed,1u>,config7>' (firmware/nnet_utils/nnet_padding_stream.h:16:59)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' to 'shift_line_buffer<array<ap_fixed,1u>,config2>' (firmware/nnet_utils/nnet_conv_stream.h:214:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' to 'relu<array,array<ap_fixed,8u>,relu_config3>' (firmware/nnet_utils/nnet_activation_stream.h:60:68)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' to 'relu<array,array<ap_fixed,16u>,relu_config6>' (firmware/nnet_utils/nnet_activation_stream.h:60:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' to 'dense_wrapper<ap_fixed,ap_fixed<16,6,5,3,0>,config5>' (firmware/nnet_utils/nnet_mult.h:24:9)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' to 'dense<array,array<ap_fixed<16,6,5,3,0>,16u>,config5>' (firmware/nnet_utils/nnet_dense_stream.h:41:39)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' to 'conv_2d_cl<array,array<ap_fixed,8u>,config2>' (firmware/nnet_utils/nnet_dense_resource.h:64:37)\n",
      "WARNING: [XFORM 203-561] Ignored multiple trip count directives for loop 'ReadInputHeight_ReadInputWidth' in function 'conv_2d_cl<array,array<ap_fixed,8u>,config2>'.\n",
      "INFO: [XFORM 203-531] Rewinding loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:64) in function 'dense_wrapper<ap_fixed,ap_fixed<16,6,5,3,0>,config5>'.\n",
      "WARNING: [XFORM 203-532] Unable to rewind loop 'ReadInputHeight_ReadInputWidth' in function 'conv_2d_cl<array,array<ap_fixed,8u>,config2>': loop nest is not flattened.\n",
      "WARNING: [XFORM 203-561] Ignored multiple trip count directives for loop 'ReuseLoop' in function 'dense_wrapper<ap_fixed,ap_fixed<16,6,5,3,0>,config5>'.\n",
      "INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:03:10 ; elapsed = 00:03:42 . Memory (MB): peak = 1083.867 ; gain = 651.090 ; free physical = 116 ; free virtual = 2052\n",
      "INFO: [HLS 200-10] Starting hardware synthesis ...\n",
      "INFO: [HLS 200-10] Synthesizing 'myproject' ...\n",
      "WARNING: [SYN 201-103] Legalizing function name 'Block__proc' to 'Block_proc'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'zeropad2d_cl<array,array<ap_fixed,1u>,config7>' to 'zeropad2d_cl_array_array_ap_fixed_1u_config7_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'shift_line_buffer<array<ap_fixed,1u>,config2>' to 'shift_line_buffer_array_ap_fixed_1u_config2_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'conv_2d_cl<array,array<ap_fixed,8u>,config2>' to 'conv_2d_cl_array_array_ap_fixed_8u_config2_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array,array<ap_fixed,8u>,relu_config3>' to 'relu_array_array_ap_fixed_8u_relu_config3_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_wrapper<ap_fixed,ap_fixed<16,6,5,3,0>,config5>' to 'dense_wrapper_ap_fixed_ap_fixed_16_6_5_3_0_config5_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense<array,array<ap_fixed<16,6,5,3,0>,16u>,config5>' to 'dense_array_array_ap_fixed_16_6_5_3_0_16u_config5_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array,array<ap_fixed,16u>,relu_config6>' to 'relu_array_array_ap_fixed_16u_relu_config6_s'.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'Block_proc' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 221.9 seconds; current allocated memory: 302.647 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.05 seconds; current allocated memory: 302.693 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'zeropad2d_cl_array_array_ap_fixed_1u_config7_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 302.924 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 303.077 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'shift_line_buffer_array_ap_fixed_1u_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'shift_line_buffer<array<ap_fixed,1u>,config2>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 303.176 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.08 seconds; current allocated memory: 303.280 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'conv_2d_cl_array_array_ap_fixed_8u_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReuseLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 305.254 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.15 seconds; current allocated memory: 306.922 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_array_ap_fixed_8u_relu_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReLUActLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.31 seconds; current allocated memory: 307.114 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 307.508 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_wrapper_ap_fixed_ap_fixed_16_6_5_3_0_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReuseLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 12.8 seconds; current allocated memory: 326.534 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 43.68 seconds; current allocated memory: 350.340 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_array_array_ap_fixed_16_6_5_3_0_16u_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'DataPrepare'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 32.66 seconds; current allocated memory: 352.494 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 3.09 seconds; current allocated memory: 354.793 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_array_ap_fixed_16u_relu_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'relu<array,array<ap_fixed,16u>,relu_config6>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 7.04 seconds; current allocated memory: 355.410 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 356.002 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 356.332 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 4.44 seconds; current allocated memory: 358.202 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'Block_proc' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'Block_proc'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.91 seconds; current allocated memory: 358.832 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'zeropad2d_cl_array_array_ap_fixed_1u_config7_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'zeropad2d_cl_array_array_ap_fixed_1u_config7_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 359.299 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'shift_line_buffer_array_ap_fixed_1u_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_1u_config2_s_line_buffer_Array_V_0_0' to 'shift_line_buffer_array_ap_fixed_1u_config2_s_line_bufferbkb' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_1u_config2_s_line_buffer_Array_V_1_0' to 'shift_line_buffer_array_ap_fixed_1u_config2_s_line_buffercud' due to the length limit 60\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'shift_line_buffer_array_ap_fixed_1u_config2_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 360.208 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'conv_2d_cl_array_array_ap_fixed_8u_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'pX' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_7' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_8' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_0' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_6' is power-on initialization.\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_16s_26_1_1': 23 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_9s_25_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'conv_2d_cl_array_array_ap_fixed_8u_config2_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 363.024 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_array_ap_fixed_8u_relu_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_array_ap_fixed_8u_relu_config3_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.65 seconds; current allocated memory: 371.539 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_wrapper_ap_fixed_ap_fixed_16_6_5_3_0_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-104] Estimated max fanout for 'dense_wrapper_ap_fixed_ap_fixed_16_6_5_3_0_config5_s' is 16375 from HDL expression: ((1'b0 == ap_block_pp0_stage0) & (1'b1 == ap_CS_fsm_pp0_stage0) & (ap_enable_reg_pp0_iter1 == 1'b1))\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_16s_26_1_1': 1023 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_7ns_16s_23_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_wrapper_ap_fixed_ap_fixed_16_6_5_3_0_config5_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 6.22 seconds; current allocated memory: 425.077 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_array_array_ap_fixed_16_6_5_3_0_16u_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_array_array_ap_fixed_16_6_5_3_0_16u_config5_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 49.67 seconds; current allocated memory: 524.807 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_array_ap_fixed_16u_relu_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_array_ap_fixed_16u_relu_config6_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 13.1 seconds; current allocated memory: 534.117 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/input_1_V_data_0_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_0_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_1_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_2_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_3_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_4_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_5_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_6_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_7_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_8_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_9_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_10_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_11_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_12_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_13_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_14_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer6_out_V_data_15_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/const_size_in_1' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/const_size_out_1' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on function 'myproject' to 'ap_ctrl_hs'.\n",
      "INFO: [SYN 201-210] Renamed object name 'start_for_dense_array_array_ap_fixed_16_6_5_3_0_16u_config5_U0' to 'start_for_dense_array_array_ap_fixed_16_6_5_3_0_16u_confidEe' due to the length limit 60\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'myproject'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.49 seconds; current allocated memory: 537.325 MB.\n",
      "INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.\n",
      "INFO: [HLS 200-789] **** Estimated Fmax: 232.61 MHz\n",
      "INFO: [RTMG 210-279] Implementing memory 'conv_2d_cl_array_array_ap_fixed_8u_config2_s_w2_V_rom' using auto ROMs.\n",
      "INFO: [RTMG 210-279] Implementing memory 'dense_wrapper_ap_fixed_ap_fixed_16_6_5_3_0_config5_s_w5_V_rom' using auto ROMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_0_V_U(fifo_w16_d81_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_0_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_1_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_2_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_3_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_4_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_5_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_6_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_7_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_0_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_1_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_2_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_3_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_4_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_5_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_6_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_7_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_conv_2d_cl_array_array_ap_fixed_8u_config2_U0_U(start_for_conv_2d_cl_array_array_ap_fixed_8u_config2_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_array_ap_fixed_8u_relu_config3_U0_U(start_for_relu_array_array_ap_fixed_8u_relu_config3_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_dense_array_array_ap_fixed_16_6_5_3_0_16u_confidEe_U(start_for_dense_array_array_ap_fixed_16_6_5_3_0_16u_confidEe)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_array_ap_fixed_16u_relu_config6_U0_U(start_for_relu_array_array_ap_fixed_16u_relu_config6_U0)' using Shift Registers.\n",
      "INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:05:52 ; elapsed = 00:07:02 . Memory (MB): peak = 1315.887 ; gain = 883.109 ; free physical = 136 ; free virtual = 1840\n",
      "INFO: [VHDL 208-304] Generating VHDL RTL for myproject.\n",
      "INFO: [VLOG 209-307] Generating Verilog RTL for myproject.\n",
      "***** C/RTL SYNTHESIS COMPLETED IN 0h6m49s *****\n",
      "INFO: [HLS 200-112] Total elapsed time: 422.37 seconds; peak allocated memory: 537.325 MB.\n",
      "INFO: [Common 17-206] Exiting vivado_hls at Tue Oct  3 17:37:17 2023...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EstimatedClockPeriod': '4.299',\n",
       " 'BestLatency': '568',\n",
       " 'WorstLatency': '569',\n",
       " 'IntervalMin': '163',\n",
       " 'IntervalMax': '568',\n",
       " 'BRAM_18K': '467',\n",
       " 'DSP48E': '1048',\n",
       " 'FF': '21470',\n",
       " 'LUT': '26768',\n",
       " 'URAM': '0',\n",
       " 'AvailableBRAM_18K': '5376',\n",
       " 'AvailableDSP48E': '12288',\n",
       " 'AvailableFF': '3456000',\n",
       " 'AvailableLUT': '1728000',\n",
       " 'AvailableURAM': '1280'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hls_model.build(csim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OutputDir': 'testingRegular',\n",
       " 'ProjectName': 'myproject',\n",
       " 'Backend': 'Vivado',\n",
       " 'XilinxPart': 'xcu250-figd2104-2L-e',\n",
       " 'Board': None,\n",
       " 'ClockPeriod': 5,\n",
       " 'IOType': 'io_stream',\n",
       " 'HLSConfig': {'Model': {'Precision': 'ap_fixed<16,6>',\n",
       "   'ReuseFactor': 1,\n",
       "   'Strategy': 'Latency'},\n",
       "  'LayerName': {'input_1': {'Precision': {'result': 'ap_fixed<16,6>'},\n",
       "    'Strategy': 'Resource',\n",
       "    'ReuseFactor': 3},\n",
       "   'conv2d': {'Precision': {'weight': 'ap_fixed<16,6>',\n",
       "     'bias': 'ap_fixed<16,6>',\n",
       "     'result': 'ap_fixed<16,6>'},\n",
       "    'ReuseFactor': 3,\n",
       "    'Strategy': 'Resource'},\n",
       "   'conv2d_relu': {'Precision': 'ap_fixed<16,6>',\n",
       "    'ReuseFactor': 3,\n",
       "    'table_size': 1024,\n",
       "    'table_t': 'ap_fixed<18,8>',\n",
       "    'Strategy': 'Resource'},\n",
       "   'encoded_vector': {'Precision': {'weight': 'ap_fixed<16,6>',\n",
       "     'bias': 'ap_fixed<16,6>',\n",
       "     'result': 'ap_fixed<16,6>'},\n",
       "    'ReuseFactor': 2,\n",
       "    'Strategy': 'Resource'},\n",
       "   'encoded_vector_relu': {'Precision': 'ap_fixed<16,6>',\n",
       "    'ReuseFactor': 3,\n",
       "    'table_size': 1024,\n",
       "    'table_t': 'ap_fixed<18,8>',\n",
       "    'Strategy': 'Resource'}}},\n",
       " 'KerasModel': <tensorflow.python.keras.engine.functional.Functional at 0x7fa6497458b0>,\n",
       " 'Stamp': '5e4D924a'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
